name: metrics
version: 1.0
created: 2025-01-11

intent: |
  What gets measured gets managed. What gets visualized gets understood.

  The metrics tool exists because developers have terrible intuition about their
  own productivity. "I worked all day" might mean 8 hours of focused coding or
  6 hours of context-switching with 2 hours of actual work. Without data, you
  can't tell the difference.

  Metrics aggregates signals from multiple sources - git commits, focus sessions,
  journal events, loop iterations - to build a picture of actual work. Not hours
  at desk, but tangible output. Lines changed, commits shipped, focus time logged.

  The deeper insight: metrics is not about MAXIMIZING output. It's about UNDERSTANDING
  patterns. Why are Mondays always slow? Why does focus time drop after lunch?
  Why do commits cluster in the evening? These patterns reveal optimization opportunities.

  The productivity score is intentionally simple and somewhat arbitrary. It's a
  conversation starter, not a performance review. "Your score was 45 today" should
  prompt reflection, not shame.

  Metrics serves the human, not the manager. All data is local. No dashboards
  for supervisors. This is YOUR data about YOUR work for YOUR improvement.

constraints:
  - All data local: No external services, no telemetry, no sharing
  - Works offline: Git and local files only, no network required
  - Read-only: Metrics observes, never modifies source data
  - Fast queries: Today's stats should return in < 1 second
  - Human-first output: ASCII charts, not raw numbers
  - Cross-platform dates: Handle macOS and Linux date commands
  - Graceful missing data: Work even if git/focus/journal data is sparse
  - Exportable: JSON and CSV output for external analysis

interface:
  commands:
    today:
      args: "[--json]"
      returns: "Today's activity summary with productivity score"
      example: "metrics today"

    week:
      args: "[--json]"
      returns: "This week's summary with daily commit chart"
      example: "metrics week"

    month:
      args: "[--json]"
      returns: "This month's summary with weekly breakdown"
      example: "metrics month"

    commits:
      args: "[--days N] [--project PATH] [--json]"
      returns: "Detailed git commit statistics"
      example: "metrics commits --days 30"

    focus:
      args: "[--days N]"
      returns: "Focus session statistics with completion rate"
      example: "metrics focus --days 14"

    activity:
      args: "[--hours N] [--category CAT]"
      returns: "Raw activity timeline from journal"
      example: "metrics activity --hours 8"

    trends:
      args: "[--days N]"
      returns: "Compare current period to previous period"
      example: "metrics trends --days 30"

    export:
      args: "[--json|--csv] [--days N]"
      returns: "Raw data export for external tools"
      example: "metrics export --csv --days 90 > productivity.csv"

  exit_codes:
    0: "Success"
    1: "Invalid arguments or command"
    2: "No data available for requested period"

examples:
  - scenario: "Quick morning check-in"
    context: "Starting the day, want to see yesterday's progress"
    action: "metrics today"
    result: "Shows git activity, events, focus sessions, productivity score"
    why_it_matters: |
      Context for the day ahead. If yesterday was highly productive, you
      might have momentum. If it was scattered, today is a fresh start.
      Either way, you're informed.

  - scenario: "Weekly team standup prep"
    context: "Need to summarize what you accomplished this week"
    action: "metrics week"
    result: "Daily commit chart, total commits, focus hours, net lines"
    why_it_matters: |
      Concrete data for standups instead of vague memories. "I made 23 commits
      this week, 4 hours of focused work on Tuesday, net +450 lines" is better
      than "I worked on the auth system."

  - scenario: "Quarterly self-review"
    context: "Performance review coming, need to document accomplishments"
    action: "metrics export --csv --days 90 > q4.csv"
    result: "CSV file with daily metrics for spreadsheet analysis"
    why_it_matters: |
      Self-advocacy requires evidence. Exporting to CSV lets you create
      charts, calculate averages, spot trends. "I averaged 12 commits/week
      with 87% focus completion rate" is compelling.

  - scenario: "Debugging productivity slump"
    context: "Feeling unproductive lately, want to understand why"
    action: "metrics trends --days 30"
    result: "Shows current 30 days vs previous 30 days, highlights changes"
    why_it_matters: |
      Sometimes you FEEL unproductive but the data says otherwise. Sometimes
      the feeling is accurate. Trends help distinguish real slumps from
      impostor syndrome.

  - scenario: "Focus optimization"
    context: "Trying to improve deep work habits"
    action: "metrics focus --days 30"
    result: "Focus sessions, completion rate, daily chart, average per day"
    why_it_matters: |
      If completion rate is 40%, your sessions are too long. If focus time
      is only 30m/day, you need more protected time. The data guides the
      intervention.

decisions:
  - choice: "Aggregate multiple data sources rather than create new tracking"
    why: |
      Metrics doesn't ask you to log anything manually. It reads what's
      already being captured:
      - Git: commits exist naturally from version control
      - Focus: sessions logged by focus tool
      - Journal: events logged by other tools

      This is zero-friction. You don't change your workflow to use metrics.
      You just run "metrics today" and see what happened.
    alternatives:
      - option: "Manual time tracking (track start/stop of tasks)"
        rejected_because: "Requires discipline, adds friction, rarely maintained"
      - option: "Automatic screen time tracking"
        rejected_because: "Privacy invasive, measures presence not productivity"
      - option: "Integration with external tools (Jira, Linear, etc.)"
        rejected_because: "Adds dependencies, requires accounts, not offline-first"

  - choice: "ASCII art charts instead of opening browser/GUI"
    why: |
      Developers live in terminals. Metrics should be glanceable without
      context-switching. A simple bar chart made of Unicode blocks:

        Mon: ████████░░░░░░░░░░░░ 12
        Tue: ██████████████░░░░░░ 21
        Wed: ████░░░░░░░░░░░░░░░░ 6

      This is enough to see patterns. Detailed analysis can use --csv export.
    alternatives:
      - option: "Open charts in browser"
        rejected_because: "Context switch, requires browser, breaks flow"
      - option: "Generate PNG/SVG images"
        rejected_because: "Extra files, requires viewer, unnecessary complexity"
      - option: "Numbers only, no visualization"
        rejected_because: "Harder to spot patterns, less engaging"

  - choice: "Simple heuristic productivity score rather than sophisticated algorithm"
    why: |
      The productivity score (0-100) is deliberately simple:
      - Commits today? +20 (activity)
      - More than 5 commits? +10 (sustained activity)
      - Completed focus sessions? +20 (intentional work)
      - 60+ min focus time? +20 (substantial depth)
      - 180+ min focus time? +10 (exceptional depth)
      - Few errors? +10 (smooth workflow)
      - Net positive lines? +10 (building vs deleting)

      This isn't scientific. It's a STARTING POINT for reflection.
      "Why is my score 35 today?" leads to useful introspection.
    alternatives:
      - option: "Sophisticated ML-based productivity model"
        rejected_because: "Black box, not actionable, false sense of precision"
      - option: "No score, just raw metrics"
        rejected_because: "Loses the summary/gamification aspect"
      - option: "User-defined score formula"
        rejected_because: "Configuration complexity, most users won't customize"

  - choice: "Compare periods (this week vs last week) for trends"
    why: |
      Absolute numbers are hard to interpret. Is 15 commits/day good?
      Depends on the project, the phase, the person.

      But "15 commits this week vs 8 commits last week" is clearly UP.
      Relative comparison removes the need for universal benchmarks.

      Your baseline is your own history.
    alternatives:
      - option: "Industry benchmarks"
        rejected_because: "Context-free, demoralizing, often meaningless"
      - option: "Goal-based tracking"
        rejected_because: "Requires setting goals, adds friction"

  - choice: "Default to 7 days for most commands"
    why: |
      A week is the natural unit of work rhythm. Most people have weekly
      patterns (meetings on Monday, deep work on Thursday, etc.). Looking
      at 7 days shows a complete cycle.

      Daily is too noisy. Monthly is too long for immediate feedback.
      Weekly is the sweet spot.
    alternatives:
      - option: "Default to 1 day"
        rejected_because: "Too narrow, misses patterns"
      - option: "Default to 30 days"
        rejected_because: "Too broad for daily check-ins"
      - option: "No default, always require --days"
        rejected_because: "Friction for most common use case"

anti_patterns:
  - pattern: "Optimizing for the metrics instead of the work"
    why_bad: |
      "I need to make more commits to raise my score" is backwards thinking.
      Commits should be logical units of work, not score-farming.

      If you split one commit into 5 to boost numbers, you've made git history
      worse and learned nothing. The metrics should REFLECT work, not DRIVE it.

  - pattern: "Comparing your metrics to others"
    why_bad: |
      Alice made 50 commits this week. Bob made 10. Who was more productive?

      Trick question. Commit count is project-dependent, style-dependent,
      phase-dependent. Alice might be making tiny fixups. Bob might have
      shipped a major feature in one commit.

      Compare yourself to yourself. Your trends matter, not rankings.

  - pattern: "Using metrics as evidence against yourself"
    why_bad: |
      "My score was only 25 today, I'm a failure" is toxic thinking.

      Low scores happen. Meetings happen. Sick days happen. Context-switch
      days happen. A 25 isn't an indictment; it's information.

      Patterns of low scores over weeks might warrant intervention.
      A single bad day is just a bad day.

  - pattern: "Checking metrics obsessively"
    why_bad: |
      Running "metrics today" every 30 minutes doesn't make you productive.
      It makes you anxious and interrupt-driven.

      Reasonable cadence: check once in the morning, once at end of day.
      Weekly review of trends. Monthly/quarterly for bigger picture.

      More frequent checking is productivity theater.

  - pattern: "Ignoring qualitative context"
    why_bad: |
      Metrics don't know that those 5 commits fixed a critical security
      vulnerability. They don't know that low activity day was spent
      mentoring a junior developer.

      Numbers are incomplete. Use them alongside memory and judgment,
      not as a replacement.

connects_to:
  - component: focus
    relationship: |
      Metrics aggregates focus session data:
        - Total sessions and completion rate
        - Daily focus time charts
        - Average focus time per day

      Focus generates the data, metrics visualizes it.

  - component: journal
    relationship: |
      Metrics reads journal events for activity timeline:
        - Event counts by category
        - Error frequency
        - Tool usage patterns

      Journal is the raw log, metrics is the aggregator.

  - component: git
    relationship: |
      Metrics uses native git for commit statistics:
        - Commit counts and frequency
        - Lines added/removed
        - Files touched
        - Contributor breakdown

      No special tracking needed - git history is the source of truth.

  - component: loop
    relationship: |
      Future: Metrics could track loop statistics:
        - Iterations per loop
        - Success/failure rates
        - Time to resolution

      This requires loop to log data, which it currently doesn't.

  - component: verify
    relationship: |
      Future: Metrics could track verification:
        - Pass/fail rates
        - Common failure types
        - Time to fix after failure

      Requires verify to log to journal.

metrics:
  success_criteria:
    - "Daily check-in becomes habit (metrics today runs regularly)"
    - "Weekly reviews reveal actionable patterns"
    - "Export used for performance reviews and self-advocacy"
    - "Trends help diagnose productivity slumps"
    - "Productivity score sparks reflection, not anxiety"

  failure_indicators:
    - "Metrics never used after initial curiosity"
    - "Users game the metrics (splitting commits, etc.)"
    - "Score causes stress instead of insight"
    - "Data collected but never reviewed"
    - "Comparisons between developers create conflict"
