name: journal
version: 1.0
created: 2025-01-11

intent: |
  "What happened?" is the most important debugging question.

  Journal exists because AI activity is ephemeral. An agent runs, makes
  changes, completes or fails - and the context is lost. Terminal scrollback
  disappears. Memory is short. "What did it do?" becomes unanswerable.

  The deeper insight: narrative is more valuable than logs. Raw events
  are overwhelming. A coherent story of "agent started, made 5 changes,
  hit error, tried 3 fixes, succeeded" is actionable.

  Journal aggregates events from ALL Daedalos tools and synthesizes them
  into human-readable narratives. It's the historian of your AI activity.

constraints:
  - Aggregate from multiple sources: loop, agent, gates, undo, mcp-hub
  - Narrative synthesis: Not just events, but coherent stories
  - Time-based queries: "Last hour", "today", "since commit"
  - Source filtering: "Show only agent events"
  - Low storage overhead: ~1MB per 10,000 events
  - Fast queries: < 500ms for typical "what happened last hour"

interface:
  commands:
    what:
      args: "[--hours N] [--days N] [--source S] [--verbose] [--json]"
      returns: "Narrative of what happened in time range"
      example: "journal what -h 2"

    events:
      args: "[--hours N] [--source S] [--type T] [--limit N] [--json]"
      returns: "Raw event list"
      example: "journal events --source loop"

    summary:
      args: "[--hours N] [--days N] [--json]"
      returns: "Statistics and highlights"
      example: "journal summary --days 1"

    log:
      args: "<message> [source] [event_type]"
      returns: "Logs a custom event"
      example: "journal log 'Started deploy' user deploy"

  sources:
    gates: "Gate checks and approvals"
    loop: "Iteration loop events (start, iterate, complete)"
    agent: "Agent lifecycle (spawn, message, signal, kill)"
    undo: "File changes and checkpoints"
    mcp-hub: "MCP server events"
    user: "Custom user-logged events"

  event_types:
    start: "Something started"
    complete: "Something completed successfully"
    fail: "Something failed"
    iterate: "Loop iteration"
    checkpoint: "Checkpoint created"
    change: "File changed"
    spawn: "Agent spawned"
    message: "Agent message sent"

  exit_codes:
    0: "Success"
    1: "Query error or no events found"
    2: "Storage error"

examples:
  - scenario: "End of day summary"
    context: "Wrapping up, want to see what AI accomplished today"
    action: "journal what --days 1"
    result: "Narrative summary of all AI activity since morning"
    why_it_matters: |
      Context reconstruction. You can't remember everything.
      Journal reconstructs the day's work.

  - scenario: "Debugging failed workflow"
    context: "Workflow failed, need to understand why"
    action: "journal events --source agent --hours 1"
    result: "Agent events leading up to failure"
    why_it_matters: |
      Postmortem analysis. See the sequence of events.
      Was it agent error? Gate denial? Timeout?

  - scenario: "Handoff to colleague"
    context: "End of shift, need to brief next person"
    action: "journal what --hours 8 > handoff.txt"
    result: "Machine-readable summary for handoff context"
    why_it_matters: |
      Collaboration tool. Journal feeds into handoff workflow.
      Continuity across humans and shifts.

  - scenario: "Checking if something ran"
    context: "Did that cron job / loop / agent actually run?"
    action: "journal events --type complete --source loop --hours 2"
    result: "Shows completed loops in time range"
    why_it_matters: |
      Verification. "Did it run?" has a clear answer.

  - scenario: "Custom event logging"
    context: "Starting a risky manual operation"
    action: "journal log 'Starting database migration' user deploy"
    result: "Event added to journal timeline"
    why_it_matters: |
      Human events mix with AI events. Full picture.
      "What happened?" includes human actions too.

decisions:
  - choice: "Narrative synthesis over raw event listing"
    why: |
      Raw events are noisy. 100 events is overwhelming.
      A narrative like:

      "In the last hour: 2 loops completed successfully, 1 agent
       spawned and is still running, 15 files changed, verify passed."

      is actionable. Numbers with context.

      The 'what' command generates narrative. 'events' gives raw.
    alternatives:
      - option: "Only raw events (log-style)"
        rejected_because: "Too noisy, hard to scan, misses patterns"
      - option: "AI-generated summaries"
        rejected_because: "Expensive, slow, overkill for local tool"

  - choice: "Time-based primary query axis"
    why: |
      "What happened in the last hour?" is the natural question.
      Time is universal - works for any event type or source.

      Filters (source, type) narrow down within time range.
    alternatives:
      - option: "Source-based primary axis"
        rejected_because: "Less natural - you rarely ask 'all loop events ever'"
      - option: "Git-commit-based ranges"
        rejected_because: "Not all work is git-tracked, limits applicability"

  - choice: "SQLite for event storage"
    why: |
      Events are:
      - Frequent writes (many per minute)
      - Time-indexed queries
      - Multi-source aggregation

      SQLite handles all this efficiently. Single file, no daemon.

      Schema: timestamp, source, event_type, summary, details_json
    alternatives:
      - option: "JSON file per day"
        rejected_because: "Slow queries, no indexing, concurrent write issues"
      - option: "Shared log file"
        rejected_because: "Hard to query, no structure, rotation complexity"

  - choice: "Pull events from tool state files"
    why: |
      Loop, agent, undo all have their own state files.
      Journal reads these to collect events, doesn't require tools
      to explicitly log to journal.

      This means journal works even if tools weren't journal-aware.

      Custom events use explicit 'journal log' command.
    alternatives:
      - option: "Tools push to journal explicitly"
        rejected_because: "Coupling, requires all tools to be journal-aware"
      - option: "Filesystem watchers"
        rejected_because: "Complex, resource-heavy, misses non-file events"

anti_patterns:
  - pattern: "Using journal as the only source of truth"
    why_bad: |
      Journal is a summary, not comprehensive logs.
      For deep debugging, check the actual tool (loop history, agent logs).

      Journal is for "what happened at a high level", not "exact command output".

  - pattern: "Querying too broad time ranges"
    why_bad: |
      "journal what --days 30" will be slow and overwhelming.
      Journal is for recent history, not long-term analytics.

      For trends, use metrics tool instead.

  - pattern: "Ignoring the narrative for raw events"
    why_bad: |
      'journal events' is there for detailed debugging.
      For regular use, 'journal what' is more useful.

      Start with narrative, drill down to events if needed.

  - pattern: "Not logging custom events for manual work"
    why_bad: |
      AI events are logged automatically. Human work isn't.
      If you do something significant manually, 'journal log' it.

      Otherwise the narrative has gaps.

connects_to:
  - component: observe
    relationship: |
      Observe shows real-time events.
      Journal shows historical events.

      Same events, different time perspectives.

  - component: gates
    relationship: |
      Gate checks are logged to journal.
      "Agent tried to git push, was denied"

      Enables audit: what did agents try to do?

  - component: loop
    relationship: |
      Loop events: start, iterate, checkpoint, complete, fail
      Journal pulls from loop state files.

      Narrative includes: "Loop 'fix tests' completed in 3 iterations"

  - component: agent
    relationship: |
      Agent events: spawn, message, signal, kill
      Journal pulls from agent state.

      Narrative includes: "Explorer agent spawned, completed exploration"

  - component: handoff
    relationship: |
      Handoff uses journal to generate context summary.
      "Here's what happened while you were away"

      Journal is the data source, handoff is the presentation.

  - component: undo
    relationship: |
      Undo events: checkpoint, restore, file changes
      Journal knows about file history.

      Narrative includes: "23 files changed, 2 checkpoints created"

metrics:
  success_criteria:
    - "'journal what -h 1' returns in < 500ms"
    - "Narrative is readable and actionable (not just data dump)"
    - "Users check journal before asking 'what happened?'"
    - "All tool events collected (no missing sources)"
    - "Storage scales reasonably (1MB per 10k events)"

  failure_indicators:
    - "Queries are slow (poor indexing)"
    - "Narrative is confusing or too verbose"
    - "Events missing from some sources"
    - "Users ignore journal (not useful enough)"
    - "Storage grows unboundedly (no cleanup)"
