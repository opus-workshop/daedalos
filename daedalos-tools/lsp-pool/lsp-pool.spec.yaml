name: lsp-pool
version: 1.0
created: 2025-01-11

intent: |
  The dirty secret of AI coding: it's bottlenecked by code intelligence startup.

  When an AI agent needs to understand code - find definitions, check types,
  get completions - it queries a Language Server. But language servers are
  notoriously slow to start. TypeScript: 10-30 seconds. Rust: 30-60 seconds.
  Large Python projects: minutes.

  This latency is invisible in IDEs because they start LSPs when you open a
  project and keep them running. But AI agents work across projects, spawn
  fresh, and can't afford to wait. The result: agents either skip LSP entirely
  (lose code intelligence) or suffer brutal startup delays.

  LSP-Pool solves this by treating language servers as a managed resource pool.
  Pre-warm what you'll likely need. Keep active servers hot. Hibernate idle ones.
  Predict based on your patterns.

  The deeper insight: code intelligence is too valuable to be gated by startup
  time. An agent with instant access to type information, definitions, and
  diagnostics makes fundamentally better decisions than one flying blind. LSP-Pool
  makes that intelligence always available.

  "Intelligence should be instant" isn't about performance metrics - it's about
  changing what's possible when latency disappears.

constraints:
  - Memory-aware: Must not consume more than configured limit (default 2GB)
  - Graceful degradation: Missing LSP returns empty, not error
  - Language-agnostic: Support any LSP-compliant server
  - Non-blocking: Queries to cold servers start warming, return partial results
  - Predict conservatively: Better to miss a prediction than over-warm
  - Works without daemon: Direct queries work, daemon adds pool management
  - Respect project boundaries: Different projects = different server instances
  - Sub-second queries: Warm server queries must complete < 100ms

interface:
  commands:
    start:
      args: "[--max-servers <n>] [--memory-limit <mb>] [--socket <path>]"
      returns: "Starts pool daemon, begins predictive warming"
      example: "lsp-pool start --max-servers 5 --memory-limit 2048"

    stop:
      args: ""
      returns: "Gracefully stops daemon and all managed servers"
      example: "lsp-pool stop"

    status:
      args: "[--json]"
      returns: "Pool health, memory usage, warm servers, queue"
      example: "lsp-pool status"

    warm:
      args: "<language> [path] [--priority <p>] [--wait]"
      returns: "Explicitly warm a server for language/project"
      example: "lsp-pool warm typescript ~/project --wait"

    cool:
      args: "<language> [path] [--hibernate]"
      returns: "Stop or hibernate a specific server"
      example: "lsp-pool cool rust --hibernate"

    query:
      args: "<command> <file:line:col> [--json] [--timeout <ms>]"
      returns: "LSP query result (hover, definition, completion, etc.)"
      example: "lsp-pool query hover src/main.ts:42:15"

    list:
      args: ""
      returns: "All registered language servers with status"
      example: "lsp-pool list"

    config:
      args: "<language> [--set KEY=VALUE]"
      returns: "View or modify language server configuration"
      example: "lsp-pool config python --set python.analysis.typeCheckingMode=strict"

    logs:
      args: "[language] [--follow] [--level <level>]"
      returns: "Server logs for debugging"
      example: "lsp-pool logs typescript -f"

    predict:
      args: ""
      returns: "Show predicted server needs with confidence scores"
      example: "lsp-pool predict"

  exit_codes:
    0: "Success"
    1: "General error"
    2: "Server not found / language not supported"
    3: "Query timeout"
    4: "Memory limit exceeded"
    5: "Server crash"
    6: "Pool not running"

examples:
  - scenario: "Agent needs type information"
    context: "AI agent wants hover info to understand a function's signature"
    action: "lsp-pool query hover src/auth.ts:150:20 --json"
    result: "Instant response with type signature, documentation, source location"
    why_it_matters: |
      Without LSP, agent guesses based on naming conventions and patterns.
      With LSP, agent has precise type information. The difference between
      "probably a string" and "User | null" changes how code gets written.

  - scenario: "Starting work on a project"
    context: "Developer cd's into a TypeScript project to start coding"
    action: "Pool detects directory change, auto-warms typescript for that path"
    result: "By the time first query arrives, server is ready"
    why_it_matters: |
      Predictive warming turns a 15-second wait into instant response.
      This is the magic of treating LSPs as a managed pool.

  - scenario: "Resource pressure"
    context: "Pool has 5 servers, 6th project needs warmup"
    action: "Pool evicts least-recently-used server, starts new one"
    result: "Active work continues, idle server hibernated"
    why_it_matters: |
      Without management, each project spawns its own LSP until system
      runs out of memory. Pool prevents resource exhaustion.

  - scenario: "Multi-language project"
    context: "Full-stack project with TypeScript frontend, Python backend, Rust CLI"
    action: "lsp-pool warm typescript . && lsp-pool warm python . && lsp-pool warm rust ."
    result: "All three servers ready, queries routed by file extension"
    why_it_matters: |
      Modern projects are polyglot. Pool manages complexity of multiple
      LSPs without manual coordination.

  - scenario: "Pattern-based prediction"
    context: "Developer always works on the webapp project at 10am"
    action: "Pool learns pattern, pre-warms typescript for ~/webapp at 9:55am"
    result: "Server ready before developer even opens the project"
    why_it_matters: |
      The best latency is when warm-up happens before you need it.
      Prediction makes cold starts invisible.

decisions:
  - choice: "One server per (language, project) pair, not global servers"
    why: |
      LSP servers index a specific project. A global TypeScript server
      would have to re-index every time you switch projects, which is
      slower than maintaining separate instances.

      The memory cost is worth the latency savings. And pool management
      ensures we don't keep too many instances alive.
    alternatives:
      - option: "Global servers that re-index on project switch"
        rejected_because: "Re-indexing can take as long as cold start, defeats the purpose"
      - option: "Single server with multi-root workspace"
        rejected_because: "Not all LSP servers support multi-root, adds complexity"

  - choice: "Hibernate as intermediate state between warm and stopped"
    why: |
      Hibernating a server (save state, kill process) uses no memory but
      allows faster restart than cold start. It's the middle ground.

      For idle servers that might be needed soon, hibernate is better than
      either keeping them running (memory) or fully stopping (slow restart).
    alternatives:
      - option: "Binary warm/stopped states only"
        rejected_because: "Loses the nuance of 'probably needed soon but not right now'"
      - option: "Suspend process (SIGSTOP)"
        rejected_because: "Still consumes memory, not portable across all servers"

  - choice: "Predict conservatively with confidence thresholds"
    why: |
      Over-prediction wastes resources and can trigger eviction cascades.
      Under-prediction just means occasional cold starts.

      Conservative prediction with 50%+ confidence threshold means we
      warm what we're reasonably sure about. The pool handles the rest.
    alternatives:
      - option: "Aggressive prediction (warm everything that might be needed)"
        rejected_because: "Resource exhaustion, defeats pool management"
      - option: "No prediction (pure on-demand)"
        rejected_because: "Loses the main benefit of proactive warming"
      - option: "User-specified warmup lists"
        rejected_because: "Cognitive load, patterns are learnable"

  - choice: "Query routing by file extension, not explicit language parameter"
    why: |
      Agents don't always know what language a file is. They know the path.
      Let the pool figure out .ts → typescript, .py → python.

      This reduces agent complexity and handles edge cases (e.g., .tsx)
      consistently.
    alternatives:
      - option: "Require explicit language parameter"
        rejected_because: "Extra cognitive load, error-prone for agents"
      - option: "Magic byte detection"
        rejected_because: "Over-engineering, extensions work 99% of the time"

  - choice: "Memory-based limits, not count-based"
    why: |
      A TypeScript server for a large project might use 800MB. A simple
      Python server might use 100MB. Counting "5 servers max" doesn't
      capture resource reality.

      Memory limits (default 2GB) let the pool make smart decisions about
      how many servers can coexist based on actual consumption.
    alternatives:
      - option: "Fixed server count limit"
        rejected_because: "Ignores that servers have wildly different memory needs"
      - option: "No limits (OS handles it)"
        rejected_because: "OOM killer is not graceful degradation"

  - choice: "Pool is optional - direct queries work without daemon"
    why: |
      Sometimes you just need one query. Starting a daemon for that is
      overkill. Direct query mode spawns the server, runs query, exits.

      Daemon mode adds: pool management, prediction, warm keeping.
      Both are valid depending on use case.
    alternatives:
      - option: "Daemon required for all operations"
        rejected_because: "Heavy for one-off queries"
      - option: "No daemon mode (pool only)"
        rejected_because: "Loses simplicity of direct queries"

anti_patterns:
  - pattern: "Warming every language 'just in case'"
    why_bad: |
      "lsp-pool warm typescript python rust go java cpp" on startup
      defeats the purpose of pool management. You're pre-allocating
      resources for languages you might never query.

      Let prediction handle warming. Explicitly warm only when you
      know you need something immediately.

  - pattern: "Ignoring memory limits"
    why_bad: |
      Setting --memory-limit 16384 on an 8GB system invites OOM.
      The pool can't manage what it can't measure.

      Set realistic limits. If you're hitting them, that's signal
      to optimize server selection, not raise limits.

  - pattern: "Querying cold servers in tight loops"
    why_bad: |
      If server isn't warm, first query triggers startup. If you
      send 100 queries before startup completes, you get 100 failures
      or 100 queued requests.

      Use --wait on first warm, or check status before bulk queries.

  - pattern: "Skipping LSP because 'it's slow'"
    why_bad: |
      The whole point of the pool is to make LSP fast. If you're
      avoiding LSP queries, you're missing code intelligence.

      Trust the pool. Use queries liberally. That's what it's for.

  - pattern: "Running multiple LSP pools"
    why_bad: |
      Each pool manages its own servers. Multiple pools = duplicate
      servers for same project = wasted memory.

      One pool per system. Multiple users? Consider user namespacing,
      not separate pools.

connects_to:
  - component: codex
    relationship: |
      Codex provides semantic code search. LSP-Pool provides precise
      code intelligence. They complement:

      - Codex: "Find code related to authentication"
      - LSP: "What's the type of this variable?"

      Agent workflows often use both: codex to find relevant code,
      LSP to understand it precisely.

  - component: loop
    relationship: |
      Loops benefit enormously from LSP. Each iteration can query
      types, check diagnostics, find definitions.

      Before loop starts, consider warming LSP for the project.
      This amortizes startup across all iterations.

  - component: verify
    relationship: |
      Verify can include LSP diagnostics as a check. "No type errors"
      is a valid verification criterion.

      lsp-pool query diagnostics . → verify checks output

  - component: project
    relationship: |
      Project info detects languages in use. This feeds prediction:
      "project has tsconfig.json" → warm TypeScript.

      Pool can query project info to make better warming decisions.

  - component: agent
    relationship: |
      Agent templates can specify LSP requirements. A "typescript-dev"
      agent template might include: warm typescript for project.

      Agents use LSP queries for code understanding, refactoring
      decisions, and implementation guidance.

  - component: mcp-hub
    relationship: |
      LSP queries can be exposed as MCP tools through the hub.
      "lsp_hover", "lsp_definition" as MCP tools lets agents
      access LSP via standard MCP protocol.

metrics:
  success_criteria:
    - "Warm server query latency < 100ms p95"
    - "Prediction accuracy > 70% for frequently-used projects"
    - "Memory stays within configured limit 99%+ of time"
    - "Agents use LSP queries regularly (not avoiding due to latency)"
    - "Cold start hidden by prediction for daily workflow patterns"

  failure_indicators:
    - "Frequent cold start latency spikes during normal work"
    - "Memory limit exceeded, servers crash"
    - "Predictions consistently wrong (warming unused servers)"
    - "Agents skip LSP queries, fly blind on types"
    - "Pool eviction thrashing (start-stop-start cycles)"
