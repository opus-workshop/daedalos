================================================================================
                         CODEX CLI BUILD PROMPT
                   Semantic Code Search v1.0
================================================================================

OVERVIEW
--------
The `codex` CLI provides natural language code search using embeddings.
Ask questions like "where is authentication handled?" and get semantically
relevant results, not just keyword matches.

================================================================================
                              YOUR TASK
================================================================================

Build a semantic code search tool using local embeddings.

USAGE:
  codex "where is user authentication handled?"
  codex "what functions call the database?"
  codex "show me error handling patterns"
  codex --file src/auth.ts "explain this file"

================================================================================
                              HOW IT WORKS
================================================================================

1. INDEXING
   - Chunk code files into meaningful segments (functions, classes, etc.)
   - Generate embeddings for each chunk using local model
   - Store embeddings in vector database

2. QUERYING
   - Embed the natural language query
   - Find nearest neighbors in vector space
   - Return ranked results with context

3. LOCAL MODELS
   - Primary: nomic-embed-text (via Ollama)
   - Fallback: sentence-transformers (Python)
   - No API keys required

================================================================================
                              ARCHITECTURE
================================================================================

codex/
  src/
    codex/
      __init__.py
      cli.py               # Click CLI
      chunker.py           # Code chunking
      embedder.py          # Embedding generation
      indexer.py           # Index management
      searcher.py          # Vector search
      models.py            # Model management
  install.sh
  README.txt

================================================================================
                              KEY IMPLEMENTATION
================================================================================

CHUNKER (src/codex/chunker.py):
-------------------------------
from dataclasses import dataclass
from typing import List
import re

@dataclass
class CodeChunk:
    file_path: str
    start_line: int
    end_line: int
    content: str
    type: str  # function, class, block, file
    name: str  # function/class name if applicable

class CodeChunker:
    def chunk_file(self, path: str, content: str) -> List[CodeChunk]:
        ext = path.split('.')[-1]
        chunker = self._get_chunker(ext)
        return chunker(path, content)

    def _chunk_python(self, path: str, content: str) -> List[CodeChunk]:
        chunks = []
        lines = content.split('\n')

        # Find function and class definitions
        pattern = r'^(class|def|async def)\s+(\w+)'
        current_chunk = None

        for i, line in enumerate(lines, 1):
            match = re.match(pattern, line)
            if match:
                if current_chunk:
                    current_chunk.end_line = i - 1
                    chunks.append(current_chunk)

                current_chunk = CodeChunk(
                    file_path=path,
                    start_line=i,
                    end_line=i,
                    content='',
                    type=match.group(1),
                    name=match.group(2)
                )

        if current_chunk:
            current_chunk.end_line = len(lines)
            chunks.append(current_chunk)

        # Fill in content
        for chunk in chunks:
            chunk.content = '\n'.join(lines[chunk.start_line-1:chunk.end_line])

        # Add file-level chunk if no functions found
        if not chunks:
            chunks.append(CodeChunk(
                file_path=path,
                start_line=1,
                end_line=len(lines),
                content=content,
                type='file',
                name=path.split('/')[-1]
            ))

        return chunks

    def _chunk_swift(self, path: str, content: str) -> List[CodeChunk]:
        # Similar logic for Swift
        pass

    def _chunk_typescript(self, path: str, content: str) -> List[CodeChunk]:
        # Similar logic for TypeScript
        pass

EMBEDDER (src/codex/embedder.py):
---------------------------------
import subprocess
import json
from typing import List

class Embedder:
    def __init__(self, model: str = "nomic-embed-text"):
        self.model = model
        self._check_model()

    def _check_model(self):
        # Check if Ollama has the model
        try:
            result = subprocess.run(
                ["ollama", "list"],
                capture_output=True, text=True
            )
            if self.model not in result.stdout:
                print(f"Pulling model {self.model}...")
                subprocess.run(["ollama", "pull", self.model])
        except FileNotFoundError:
            raise RuntimeError("Ollama not found. Install from https://ollama.ai")

    def embed(self, text: str) -> List[float]:
        result = subprocess.run(
            ["ollama", "embed", self.model, text],
            capture_output=True, text=True
        )
        return json.loads(result.stdout)['embedding']

    def embed_batch(self, texts: List[str]) -> List[List[float]]:
        return [self.embed(t) for t in texts]

INDEXER (src/codex/indexer.py):
-------------------------------
import sqlite3
import json
from pathlib import Path
from .chunker import CodeChunker
from .embedder import Embedder

class CodeIndex:
    def __init__(self, project_path: str):
        self.project_path = Path(project_path).resolve()
        self.cache_dir = Path.home() / '.cache' / 'claude-os' / 'codex'
        self.db_path = self.cache_dir / f"{self._hash_path()}.db"
        self.chunker = CodeChunker()
        self.embedder = Embedder()
        self._init_db()

    def _init_db(self):
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.conn = sqlite3.connect(self.db_path)
        self.conn.execute('''
            CREATE TABLE IF NOT EXISTS chunks (
                id INTEGER PRIMARY KEY,
                file_path TEXT,
                start_line INTEGER,
                end_line INTEGER,
                content TEXT,
                type TEXT,
                name TEXT,
                embedding BLOB
            )
        ''')
        self.conn.execute('''
            CREATE TABLE IF NOT EXISTS metadata (
                key TEXT PRIMARY KEY,
                value TEXT
            )
        ''')
        self.conn.commit()

    def index_project(self):
        print("Indexing project...")
        for path in self.project_path.rglob('*'):
            if path.is_file() and self._should_index(path):
                self._index_file(path)
        print("Indexing complete.")

    def _index_file(self, path: Path):
        content = path.read_text(errors='ignore')
        rel_path = str(path.relative_to(self.project_path))
        chunks = self.chunker.chunk_file(rel_path, content)

        for chunk in chunks:
            # Create embedding
            embedding = self.embedder.embed(chunk.content[:2000])  # Truncate

            # Store
            self.conn.execute('''
                INSERT INTO chunks (file_path, start_line, end_line, content, type, name, embedding)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (chunk.file_path, chunk.start_line, chunk.end_line,
                  chunk.content, chunk.type, chunk.name,
                  json.dumps(embedding)))

        self.conn.commit()

SEARCHER (src/codex/searcher.py):
---------------------------------
import json
import numpy as np
from typing import List, Tuple

class CodeSearcher:
    def __init__(self, index: 'CodeIndex'):
        self.index = index
        self.embedder = index.embedder

    def search(self, query: str, limit: int = 10) -> List[dict]:
        # Embed query
        query_embedding = np.array(self.embedder.embed(query))

        # Get all embeddings
        cursor = self.index.conn.execute(
            'SELECT id, file_path, start_line, end_line, content, name, embedding FROM chunks'
        )

        results = []
        for row in cursor:
            chunk_embedding = np.array(json.loads(row[6]))
            similarity = self._cosine_similarity(query_embedding, chunk_embedding)
            results.append({
                'id': row[0],
                'file_path': row[1],
                'start_line': row[2],
                'end_line': row[3],
                'content': row[4],
                'name': row[5],
                'similarity': similarity
            })

        # Sort by similarity
        results.sort(key=lambda x: x['similarity'], reverse=True)
        return results[:limit]

    def _cosine_similarity(self, a: np.ndarray, b: np.ndarray) -> float:
        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

CLI (src/codex/cli.py):
-----------------------
import click
from .indexer import CodeIndex
from .searcher import CodeSearcher

@click.command()
@click.argument('query')
@click.option('--project', '-p', default='.', help='Project path')
@click.option('--limit', '-n', default=5, help='Number of results')
@click.option('--reindex', is_flag=True, help='Force reindex')
def main(query, project, limit, reindex):
    """Semantic code search."""
    index = CodeIndex(project)

    if reindex or not index.is_indexed():
        index.index_project()

    searcher = CodeSearcher(index)
    results = searcher.search(query, limit)

    for i, r in enumerate(results, 1):
        pct = r['similarity'] * 100
        click.echo(f"\n{i}. [{pct:.0f}%] {r['file_path']}:{r['start_line']}-{r['end_line']}")
        click.echo(f"   {r['name']} ({r['content'][:100]}...)")

================================================================================
                              OUTPUT EXAMPLE
================================================================================

$ codex "where is user authentication handled?"

1. [94%] src/auth/AuthService.swift:23-89
   handleLogin (Handles user login with email/password...)

2. [87%] src/auth/TokenManager.swift:12-45
   validateToken (Validates JWT token and refreshes...)

3. [82%] src/views/LoginView.swift:34-120
   LoginView (SwiftUI view for user login form...)

4. [76%] src/middleware/AuthMiddleware.swift:8-34
   authenticate (Middleware to check auth headers...)

5. [71%] src/models/User.swift:1-45
   User (User model with auth-related fields...)

================================================================================
                              REQUIREMENTS
================================================================================

- Ollama (https://ollama.ai) for local embeddings
- OR sentence-transformers Python package as fallback
- sqlite3 (standard library)
- numpy for vector operations

================================================================================
                              BUILD CHECKLIST
================================================================================

[ ] Create directory structure
[ ] Implement chunker for Python, Swift, TypeScript, Go, Rust
[ ] Implement embedder with Ollama integration
[ ] Implement indexer with SQLite storage
[ ] Implement searcher with cosine similarity
[ ] Create CLI
[ ] Add progress indicators for indexing
[ ] Handle large codebases efficiently
[ ] Write tests
[ ] Create install.sh
[ ] Write README.txt

================================================================================
