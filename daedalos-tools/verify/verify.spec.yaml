name: verify
version: 1.0
created: 2025-01-11

intent: |
  One command. All checks. No excuses.

  The verify tool exists because verification is fragmented. Every project
  has a different incantation: "npm run lint && npm run typecheck && npm test"
  vs "cargo clippy && cargo test" vs "ruff check . && mypy . && pytest".

  This fragmentation is annoying for humans and deadly for AI agents.
  An agent doesn't know your project's magic verification spell.
  Verify provides a universal interface: just run "verify".

  The deeper insight: verification is a PROMISE. When a loop needs to know
  "am I done?", the answer is "verify --quick" or just "verify".
  This makes verify the default promise for most loops.

  Evidence before assertions. Run verify before claiming anything works.

constraints:
  - Zero configuration required: Must work out-of-box via auto-detection
  - Project-specific config optional: .daedalos/verify.yaml for overrides
  - Pipeline detection < 500ms: Fast startup is critical
  - Step timeout configurable: Default 600s, some builds are slow
  - Must continue after failures: Show ALL problems, not just first
  - Parallel steps when possible: Build and test can't parallel, but lints can
  - Clean exit codes: 0 = pass, 1 = fail, 2+ = operational errors
  - Works offline: No network calls except what the underlying tools need

interface:
  commands:
    default:
      args: "[path] [--quick] [--staged] [--watch] [--fix] [--json]"
      returns: "Verification result with step-by-step status"
      example: "verify"

    quick:
      args: "[path]"
      returns: "Fast verification (lint + types only, skip build/test)"
      example: "verify --quick"

    staged:
      args: "[path]"
      returns: "Verify only git-staged files"
      example: "verify --staged"

    watch:
      args: "[path]"
      returns: "Continuous verification with status bar"
      example: "verify --watch"

    fix:
      args: "[path]"
      returns: "Auto-fix linting issues, re-verify"
      example: "verify --fix"

    status:
      args: ""
      returns: "Last verification result and detected pipeline"
      example: "verify status"

    pipelines:
      args: ""
      returns: "List available verification pipelines"
      example: "verify pipelines"

    init:
      args: ""
      returns: "Create project-specific verify config"
      example: "verify init"

  exit_codes:
    0: "All checks passed"
    1: "One or more checks failed"
    2: "Configuration error"
    3: "Required tool not found"
    4: "Timeout exceeded"

  pipelines:
    typescript:
      steps: ["lint (eslint)", "types (tsc)", "build (npm)", "test (npm)"]
    python:
      steps: ["lint (ruff)", "types (mypy)", "test (pytest)"]
    rust:
      steps: ["lint (clippy)", "types (check)", "build (cargo)", "test (cargo)"]
    go:
      steps: ["lint (vet)", "build (go)", "test (go)"]
    swift:
      steps: ["lint (swiftlint)", "types (swift build)", "build (xcodebuild)", "test (xcodebuild)"]
    shell:
      steps: ["lint (shellcheck)", "format (shfmt)"]

examples:
  - scenario: "Quick check while coding"
    context: "Developer making changes, wants fast feedback"
    action: "verify --quick"
    result: "Lint and type check in ~2 seconds, skip slow build/test"
    why_it_matters: |
      Tight feedback loop. Know immediately if your types are wrong
      without waiting for full test suite. Quick enough to run on save.

  - scenario: "Pre-commit verification"
    context: "About to commit, want to check only staged changes"
    action: "verify --staged"
    result: "Lint/type check only on files in staging area"
    why_it_matters: |
      Don't fail commit for unrelated issues in unstaged files.
      Focused verification on exactly what you're committing.

  - scenario: "Loop promise"
    context: "Loop running 'fix the failing tests'"
    action: "--promise 'verify'"
    result: "Loop iterates until full verification passes"
    why_it_matters: |
      Universal promise that works for any project. No need to know
      if it's npm test or pytest or cargo test - just "verify".

  - scenario: "Auto-fix lint issues"
    context: "Many small lint violations after large refactor"
    action: "verify --fix"
    result: "Auto-fixes what's possible (formatting, imports), re-runs verify"
    why_it_matters: |
      Don't waste time on mechanical fixes. Let tools handle the boring stuff.
      Then focus on the real issues that need thought.

  - scenario: "Continuous verification"
    context: "Long coding session, want ambient feedback"
    action: "verify --watch"
    result: "Runs quick on file changes, shows persistent status bar"
    why_it_matters: |
      Verification becomes ambient. You see problems as they happen,
      not 10 minutes later when you finally run checks.

  - scenario: "CI/CD pipeline"
    context: "Automated build in GitHub Actions"
    action: "verify --json"
    result: "JSON output for programmatic parsing, non-zero exit on failure"
    why_it_matters: |
      Same verify command works locally and in CI. No separate CI config
      that drifts from local checks.

decisions:
  - choice: "Auto-detect pipeline from project files, not file extensions"
    why: |
      File extensions are ambiguous (.ts could be in a Node project or Deno).
      Project files (package.json, Cargo.toml, pyproject.toml) are definitive.

      Detection order:
      1. .daedalos/verify.yaml (explicit config)
      2. package.json → typescript/javascript
      3. Cargo.toml → rust
      4. pyproject.toml / setup.py → python
      5. go.mod → go
      6. Package.swift / *.xcodeproj → swift

      First match wins. Explicit config always overrides.
    alternatives:
      - option: "Require explicit pipeline config"
        rejected_because: "Zero-config is the goal, config should be optional"
      - option: "Detect from file extensions"
        rejected_because: "Ambiguous, doesn't tell you which tools are configured"
      - option: "Ask user on first run"
        rejected_because: "Interactive prompts break automation"

  - choice: "Continue after step failure (show all problems)"
    why: |
      If lint fails, still run types, build, test. User wants to know
      ALL the problems, not discover them one-by-one on each run.

      This is different from typical build systems that fail-fast.
      Verification is diagnostic - give complete picture.
    alternatives:
      - option: "Fail-fast on first error"
        rejected_because: "Wastes time, user fixes lint, runs again, discovers type error, repeat"
      - option: "Configurable continue/stop"
        rejected_because: "Complexity for little benefit, continue is almost always right"

  - choice: "Quick mode is lint + types, never build + test"
    why: |
      "Quick" means < 5 seconds. Build and test can take minutes.

      Lint catches style issues, types catch logic errors.
      These are the high-value, low-cost checks. Run them constantly.

      Build/test are for full verification before commit/push.
    alternatives:
      - option: "Quick = everything in parallel"
        rejected_because: "Still slow, parallel doesn't help for sequential dependencies"
      - option: "Configurable quick steps"
        rejected_because: "Complexity. Quick means quick. Use --step for custom."

  - choice: "YAML for pipeline definitions, not code"
    why: |
      Pipelines are declarative: step name, command, timeout, fix command.
      YAML is readable, versionable, shareable.

      Built-in pipelines in /pipelines/*.yaml.
      Project overrides in .daedalos/verify.yaml.
    alternatives:
      - option: "JavaScript/Python pipeline plugins"
        rejected_because: "Overkill. YAML covers 99% of cases."
      - option: "INI-style config"
        rejected_because: "Can't express lists and nested config cleanly"

  - choice: "Step timeouts default to 600s (10 minutes)"
    why: |
      Large test suites genuinely take this long. Timeout is safety net
      for hung processes, not aggressive limit.

      Per-step timeouts configurable for projects with known limits:
        test:
          timeout: 300  # 5 minutes max

      Quick mode has implicit low timeout (30s) since it's meant to be fast.
    alternatives:
      - option: "No timeout (let it run forever)"
        rejected_because: "Hung processes block loops forever"
      - option: "Short default (60s)"
        rejected_because: "Many legitimate test suites exceed 60s"

anti_patterns:
  - pattern: "Skipping verify before claiming 'it works'"
    why_bad: |
      "I ran the function and it worked" is not verification.
      Lint catches things you didn't notice. Types catch edge cases.
      Tests catch regressions.

      verify is the contract. Run it.

  - pattern: "Using --skip-step liberally"
    why_bad: |
      "verify --skip lint --skip types" means you're not verifying.
      If a step is genuinely wrong for your project, configure it out
      in .daedalos/verify.yaml. Don't skip on command line.

      Skips should be rare exceptions, not workflow.

  - pattern: "Running full verify on every keystroke"
    why_bad: |
      Full verify can take minutes. Running it constantly kills flow.

      Use --quick for rapid iteration, --watch for ambient feedback,
      full verify for checkpoints (commit, push, PR).

  - pattern: "Different verification in CI vs local"
    why_bad: |
      "Works on my machine" becomes "verify passes locally but fails in CI".

      Use the same verify command everywhere. If CI needs special config,
      put it in .daedalos/verify.yaml so local picks it up too.

  - pattern: "Ignoring verify failures 'because it's just a warning'"
    why_bad: |
      Warnings are errors waiting to happen. --max-warnings 0 is intentional.

      If a warning is genuinely acceptable, configure the tool to ignore it
      (eslint-disable, # noqa, etc.). Don't train yourself to ignore red.

connects_to:
  - component: loop
    relationship: |
      Verify is THE default promise for loops:
        loop start "fix the bug" --promise "verify"

      --quick for fast iteration loops:
        loop start "fix types" --promise "verify --quick"

      Loop + verify is the core workflow of Daedalos.

  - component: undo
    relationship: |
      When verify fails after edits:
        "Verification failed. Run 'undo last' to revert?"

      Verify failure + undo last = instant recovery from broken state.

  - component: gates
    relationship: |
      Gates can require verify pass before operations:
        git_commit: requires verify
        git_push: requires verify

      This enforces "evidence before commit" culture.

  - component: journal
    relationship: |
      Verify results logged to journal:
        - verify passed → journal log "verify: passed in 8.3s"
        - verify failed → journal log "verify: failed at types (3 errors)"

      Journal shows verification history over time.

  - component: observe
    relationship: |
      Observe dashboard shows verify status:
        [Last verify: 2m ago ✅] or [Last verify: 5m ago ❌ types]

      Ambient visibility into verification state.

  - component: notify
    relationship: |
      verify --watch can notify on status change:
        - Was passing, now failing → desktop notification
        - Was failing, now passing → celebration notification

      Ambient awareness without staring at terminal.

metrics:
  success_criteria:
    - "Pipeline detection works correctly for 95%+ of common project types"
    - "--quick completes in < 5 seconds for typical projects"
    - "Zero-config experience works (no .daedalos/verify.yaml needed)"
    - "verify is the default promise for loops (users choose it naturally)"
    - "Same verify command works locally and in CI"

  failure_indicators:
    - "Users write custom verification scripts instead of using verify"
    - "Users always need .daedalos/verify.yaml (detection failing)"
    - "--quick is too slow (> 10 seconds consistently)"
    - "CI has different verify config than local (drift)"
    - "verify failures ignored or skipped routinely"
